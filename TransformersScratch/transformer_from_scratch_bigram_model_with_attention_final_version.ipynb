{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da59bc2",
   "metadata": {
    "id": "0da59bc2"
   },
   "source": [
    "### Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced6a19",
   "metadata": {
    "id": "dced6a19"
   },
   "outputs": [],
   "source": [
    "with open('/content/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92145816",
   "metadata": {
    "id": "92145816"
   },
   "source": [
    "### Data exploration and preprocessing of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7317e71",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7317e71",
    "outputId": "a4270149-2cf5-43c2-fd42-c3eb8328671c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the text is: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of the text is: {}\".format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96564507",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96564507",
    "outputId": "605befe8-90ae-4e23-bdbd-36e825bf0913"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada553ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ada553ab",
    "outputId": "e28b9d7a-22fb-4254-ad64-d66930b3b437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print(\"\".join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef55feba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ef55feba",
    "outputId": "5688c1b0-7fcd-48eb-d465-4c82522731f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size is: 65\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(chars)\n",
    "print(\"Vocabulary size is: {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8b554b",
   "metadata": {
    "id": "8b8b554b"
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "There are different ways to tokenize the text. Google uses SentencePiece tokenizer mechanism [Link](https://github.com/google/sentencepiece) that uses subwords to tokenize the sentences. OpenAI which created chatCPT uses tiktoker library developed by them [Link](https://github.com/openai/tiktoken) that uses a BPE (byte-pair encoding) to tokenize.\n",
    "\n",
    "For this work we will be using character level encoding (converting the text to their ASCII equivalent).\n",
    "\n",
    "There is a tradeoff between the encoding numbers (below it is the ASCII codes) and the sequence length generated (a code for every character below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85398a9f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85398a9f",
    "outputId": "3a1d0702-d58c-4afb-c5d1-5d2170d63829"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding for the string \"hi there\" is: [46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "Decoding for the string \"hi there\" is: hii there\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch: i for i,ch in enumerate(chars)}\n",
    "itos = {i: ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "print(\"Encoding for the string \\\"hi there\\\" is: {}\".format(encode(\"hii there\")))\n",
    "print(\"Decoding for the string \\\"hi there\\\" is: {}\".format(decode(encode(\"hii there\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd6838",
   "metadata": {
    "id": "e8dd6838"
   },
   "source": [
    "### Encoding the entire data and using storing them as tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b910f6",
   "metadata": {
    "id": "22b910f6"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28369d08",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28369d08",
    "outputId": "29e269d0-a408-4b64-b74a-c8fa0086f724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f25df6",
   "metadata": {
    "id": "81f25df6"
   },
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3706577",
   "metadata": {
    "id": "f3706577"
   },
   "outputs": [],
   "source": [
    "train_data_len = int(0.9 * len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff05ae",
   "metadata": {
    "id": "48ff05ae"
   },
   "outputs": [],
   "source": [
    "train_data = data[:train_data_len]\n",
    "val_data = data[train_data_len:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35ed023",
   "metadata": {
    "id": "d35ed023"
   },
   "source": [
    "### Batch size and batches for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcda515",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bcda515",
    "outputId": "8a2b34a1-c456-4647-d68e-8039663b322e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
       "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
       "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
       "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
       "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
       "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
       "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
       "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
       "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
       "        50, 50, 10,  0, 35])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the chunk of data that will be fed to model at once, this might be known as context-length in other cases\n",
    "\n",
    "batch_size = 64 # how many independent sequences will we process in parellel?\n",
    "block_size = 256 # What is the maximum length allowed for prediction?\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "eval_interval = 500\n",
    "max_iters = 5000\n",
    "n_embed = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# if learning rate is lower, number of iterations should be higher\n",
    "lr = 3e-4\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00bc379",
   "metadata": {
    "id": "d00bc379"
   },
   "outputs": [],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "# for i,c in enumerate(x):\n",
    "#     print(\"When the context is {}, the most probable next data is {}\".format(x[:i+1], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a684a944",
   "metadata": {
    "id": "a684a944"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of input x and outputs y\n",
    "    data = train_data if split==\"train\" else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd32ac1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddd32ac1",
    "outputId": "faeb450c-fba3-4b9d-ea54-9d64e4af8c80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs are tensor([[ 0, 26, 53,  ..., 56, 43, 47],\n",
      "        [60, 43, 56,  ..., 56,  1, 41],\n",
      "        [26, 21, 33,  ..., 26, 21, 13],\n",
      "        ...,\n",
      "        [ 5, 57,  1,  ...,  1, 35, 47],\n",
      "        [56, 53, 53,  ..., 59, 50, 42],\n",
      "        [42, 47, 56,  ..., 39, 56,  1]], device='cuda:0') with shape torch.Size([64, 256])\n",
      "Outputs are tensor([[26, 53, 58,  ..., 43, 47, 45],\n",
      "        [43, 56,  1,  ...,  1, 41, 53],\n",
      "        [21, 33, 31,  ..., 21, 13, 10],\n",
      "        ...,\n",
      "        [57,  1, 52,  ..., 35, 47, 50],\n",
      "        [53, 53, 58,  ..., 50, 42,  1],\n",
      "        [47, 56, 43,  ..., 56,  1, 51]], device='cuda:0') with shape torch.Size([64, 256])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch(\"train\")\n",
    "print(f\"Inputs are {xb} with shape {xb.shape}\")\n",
    "print(f\"Outputs are {yb} with shape {yb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c86a15c",
   "metadata": {
    "id": "8c86a15c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "#         print(f\"When input is {context.tolist()} then most probable output is {target.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b65597e",
   "metadata": {
    "id": "1b65597e"
   },
   "source": [
    "### Baseline mode - Bigram model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d11b9d5",
   "metadata": {
    "id": "01702602"
   },
   "source": [
    "### Attention\n",
    "\n",
    "1. Attention is a communication mechanism\n",
    "2. There is no notion of space, they are just vectors without any idea of dimension thats why we add positional encoding.\n",
    "3. Every batch is independently trained and they never talk to each other.\n",
    "4. \"Self attention\" - The key, query and value matrix are all coming from the same source.\n",
    "5. \"Cross attention\" - When we pull matrix key and value from different nodes, this is called cross attention.\n",
    "6. If we have unit gaussian input when we set our matrix, the variance of the resultant matrix will be of the order of head_size which is far from when we had started setting the matrix up. If we multiply by sqrt(head_size) then we preserve information. This is important so that the the result after softmax is not sharpened towards the maximum value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac42c01",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9cc609",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e9cc609",
    "outputId": "b13d53fe-3259-467f-ecc5-0f57239dc6e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7edf18081c50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce90c2c",
   "metadata": {
    "id": "fce90c2c"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8c6d5d",
   "metadata": {
    "id": "cb8c6d5d"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "\n",
    "        k = self.key(x) # (B,T,16)\n",
    "        q = self.query(x) # (B,T,16)\n",
    "\n",
    "        # compute attention score (\"affinities to other tokens around\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B,T,16) @ (B,16,T) => (B,T,T)\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf')) # (B,T,T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B,T,T)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # perform the aggrrgation\n",
    "        v = self.value(x) # (B,T,C)\n",
    "\n",
    "        out = wei @ v # (B,T,T) @ (B,T,C) => (B,T,C)\n",
    "\n",
    "        out.shape\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a42de72",
   "metadata": {
    "id": "7a42de72"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5873e0",
   "metadata": {
    "id": "ec5873e0"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4*n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embed, n_embed),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b412c",
   "metadata": {
    "id": "6f8b412c"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Communication followed by computation\"\"\"\n",
    "    def __init__(self,n_embed, n_head):\n",
    "        # n_embed => embedding dimension\n",
    "        # n_head => number of heads we would like\n",
    "        super().__init__()\n",
    "        head_size = n_embed//n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # in original paper, LayerNorm is applied to the output of Multi head attention and FF, but in recent times\n",
    "        # there is slight deviation and it is applied to the input of those components\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a6b5ab",
   "metadata": {
    "id": "e4a6b5ab",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BiGramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # each token directly reads the logits of the next token from lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        # position embedding table\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        # Multiple blocks of self attention head\n",
    "\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed) # final layer norm\n",
    "\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "\n",
    "        # idx and target are both (B,T) tensors of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C) => (Batch, Time, Channel=n_embed) => (4,8,vocab_size)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        # cross entropy expects the output as (B, C, T), so we need to reshape\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # we need to crop idx because our embedding size is limited to block size now\n",
    "            idx_cropped = idx[:,-block_size:]\n",
    "\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cropped)\n",
    "            # focus only on the last tim step\n",
    "            logits = logits[:,-1,:] # becomes (B,C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # (B,1)\n",
    "            # append sampled index to running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B,T+1)\n",
    "        return idx\n",
    "\n",
    "model = BiGramLanguageModel()\n",
    "m = model.to(device)\n",
    "# logits, loss = m(xb,yb)\n",
    "# print(logits.shape)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d754485a",
   "metadata": {
    "id": "d754485a"
   },
   "source": [
    "We can actually predict the log liklihood of the data if we know the vocab size, it is `-ln(1/vocab_size)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b3db51",
   "metadata": {
    "id": "96b3db51"
   },
   "source": [
    "#### Now let us create the model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1926ffd4",
   "metadata": {
    "id": "1926ffd4"
   },
   "outputs": [],
   "source": [
    "# create a Pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y7irIh_uQfFQ",
   "metadata": {
    "id": "y7irIh_uQfFQ"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ab46af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81ab46af",
    "outputId": "c0705e49-c2c5-4176-9339-105b678f1c3c",
    "scrolled": true
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.1455, val loss 2.1991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 500/5000 [05:15<36:51,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 1.6804, val loss 1.8427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1000/5000 [10:29<32:35,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: train loss 1.4990, val loss 1.6905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1500/5000 [15:43<28:39,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1500: train loss 1.3918, val loss 1.6107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2000/5000 [20:57<24:26,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000: train loss 1.3228, val loss 1.5605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2500/5000 [26:11<20:20,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2500: train loss 1.2641, val loss 1.5290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3000/5000 [31:24<16:17,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000: train loss 1.2193, val loss 1.5142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 3500/5000 [36:37<12:12,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3500: train loss 1.1813, val loss 1.4960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4000/5000 [41:51<08:08,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4000: train loss 1.1421, val loss 1.4920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 4500/5000 [47:04<04:03,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4500: train loss 1.1045, val loss 1.4863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [52:17<00:00,  1.59it/s]\n"
     ]
    }
   ],
   "source": [
    "for iter in tqdm(range(max_iters)):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d71bac1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d71bac1",
    "outputId": "f5ccbd73-58de-40e1-c58f-40badcda16b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROMEO:\n",
      "But believe thy faith intertaints\n",
      "Even leave in post, blows, revengeant\n",
      "To this do tempt adversart death.\n",
      "\n",
      "MERCUTIO:\n",
      "I am glieve with Fourth, great, tender'd Rates from the poor\n",
      "And bear honour!\n",
      "\n",
      "ROMEO:\n",
      "\n",
      "MERCUTIO:\n",
      "Apon that will we may be your to arm this lave\n",
      "And beards.\n",
      "This three word gnarly thine: hark you go'd,--what liers?\n",
      "How dost thou art thou wife! I see her merving blood\n",
      "By now his name with use and linking,\n",
      "And believed; I here hurrand. Loverly then\n",
      "I did empt you to do him poison with thy master's heavy:\n",
      "That every are made's own gentleman\n",
      "To there oratom'd willieves an eyes, he loved at his\n",
      "general intent, they had as to-day; to take it.\n",
      "\n",
      "BENVOLIO:\n",
      "This my life, and my society as we are judged means,\n",
      "I have died together'd interror herewit.\n",
      "O, thus, say to-day.\n",
      "\n",
      "ANGELO:\n",
      "Barehold:\n",
      "Then lie to the breath, and more.\n",
      "\n",
      "First Gentleman:\n",
      "She lady; I singly come the army thing: the nurse of veiolenes\n",
      "Masters madely bury in the beams treadful bow.\n",
      "How faresh receives? why fo\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx=torch.zeros((1,1), dtype = torch.long, device = device), max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110cfc9",
   "metadata": {
    "id": "0110cfc9"
   },
   "source": [
    "# Random testing (Can ignore this section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe6c2c7",
   "metadata": {
    "id": "9fe6c2c7",
    "outputId": "0e1c925c-ea32-43c8-849c-24b96f5789c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe4e89",
   "metadata": {
    "id": "f8fe4e89"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce74c9",
   "metadata": {
    "id": "6fce74c9"
   },
   "outputs": [],
   "source": [
    "# version 1\n",
    "# xbow is x bag of words\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] #(t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502e4ac1",
   "metadata": {
    "id": "502e4ac1",
    "outputId": "39d3a1c5-a8af-4738-8b6e-57903b9c8830"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d9d79",
   "metadata": {
    "id": "899d9d79",
    "outputId": "765acbd3-6878-4a26-89c4-d1e0ef5f2ab7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95688ba4",
   "metadata": {
    "id": "95688ba4"
   },
   "source": [
    "The above method is inefficient as we are doing it in n^2 complexity, we can use matrix multiplication to lower the time complexity. We will use the lower triagular matrix and make it more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb677c",
   "metadata": {
    "id": "a6fb677c",
    "outputId": "9f68d907-1f64-46b5-fdf3-5b0fe921a8a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2\n",
    "\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x\n",
    "# both the torch vectors are same, this is the more efficient way\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a5f237",
   "metadata": {
    "id": "64a5f237",
    "outputId": "6d37ab27-8d73-4bd8-8b33-35082fa8f767"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b0710",
   "metadata": {
    "id": "f31b0710",
    "outputId": "1962d881-3f22-4c04-fee5-0cba63fde956"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4\n",
    "# Self attention !\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape\n",
    "\n",
    "# single head perform self attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B,T,16)\n",
    "q = query(x) # (B,T,16)\n",
    "\n",
    "wei = q @ k.transpose(-2,-1) # (B,T,16) @ (B,16,T) => (B,T,T)\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "\n",
    "# wei is equivalent to the np.dot(Q,K)\n",
    "#wei comes from Q.K_transpose\n",
    "# wei = torch.zeros((T,T))\n",
    "\n",
    "# this line of code prevents future nodes to communicate information to past nodes. If removed every node(token)\n",
    "# will interact with each other. This is removed on the encoder side of the transformers.\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a27e2f",
   "metadata": {
    "id": "a9a27e2f",
    "outputId": "6e1feb81-5a53-486a-feef-d4716a58c2e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d2f94",
   "metadata": {
    "id": "f06d2f94"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
