{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da59bc2",
   "metadata": {},
   "source": [
    "### Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dced6a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92145816",
   "metadata": {},
   "source": [
    "### Data exploration and preprocessing of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7317e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the text is: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of the text is: {}\".format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96564507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ada553ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print(\"\".join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef55feba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size is: 65\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(chars)\n",
    "print(\"Vocabulary size is: {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8b554b",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "There are different ways to tokenize the text. Google uses SentencePiece tokenizer mechanism [Link](https://github.com/google/sentencepiece) that uses subwords to tokenize the sentences. OpenAI which created chatCPT uses tiktoker library developed by them [Link](https://github.com/openai/tiktoken) that uses a BPE (byte-pair encoding) to tokenize.\n",
    "\n",
    "For this work we will be using character level encoding (converting the text to their ASCII equivalent).\n",
    "\n",
    "There is a tradeoff between the encoding numbers (below it is the ASCII codes) and the sequence length generated (a code for every character below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85398a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding for the string \"hi there\" is: [46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "Decoding for the string \"hi there\" is: hii there\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch: i for i,ch in enumerate(chars)}\n",
    "itos = {i: ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "print(\"Encoding for the string \\\"hi there\\\" is: {}\".format(encode(\"hii there\")))\n",
    "print(\"Decoding for the string \\\"hi there\\\" is: {}\".format(decode(encode(\"hii there\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd6838",
   "metadata": {},
   "source": [
    "### Encoding the entire data and using storing them as tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22b910f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28369d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f25df6",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3706577",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_len = int(0.9 * len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48ff05ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:train_data_len]\n",
    "val_data = data[train_data_len:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35ed023",
   "metadata": {},
   "source": [
    "### Batch size and batches for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bcda515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the chunk of data that will be fed to model at once, this might be known as context-length in other cases\n",
    "\n",
    "batch_size = 32 # how many independent sequences will we process in parellel?\n",
    "block_size = 8 # What is the maximum length allowed for prediction?\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "max_iters = 10000\n",
    "eval_interval = 1000\n",
    "n_embed = 32\n",
    "# if learning rate is lower, number of iterations should be higher\n",
    "lr = 1e-3\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d00bc379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the context is tensor([18]), the most probable next data is 47\n",
      "When the context is tensor([18, 47]), the most probable next data is 56\n",
      "When the context is tensor([18, 47, 56]), the most probable next data is 57\n",
      "When the context is tensor([18, 47, 56, 57]), the most probable next data is 58\n",
      "When the context is tensor([18, 47, 56, 57, 58]), the most probable next data is 1\n",
      "When the context is tensor([18, 47, 56, 57, 58,  1]), the most probable next data is 15\n",
      "When the context is tensor([18, 47, 56, 57, 58,  1, 15]), the most probable next data is 47\n",
      "When the context is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the most probable next data is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for i,c in enumerate(x):\n",
    "    print(\"When the context is {}, the most probable next data is {}\".format(x[:i+1], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a684a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of input x and outputs y\n",
    "    data = train_data if split==\"train\" else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddd32ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs are tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54],\n",
      "        [57, 43, 60, 43, 52,  1, 63, 43],\n",
      "        [60, 43, 42,  8,  0, 25, 63,  1],\n",
      "        [56, 42,  5, 57,  1, 57, 39, 49],\n",
      "        [43, 57, 58, 63,  6,  1, 58, 46],\n",
      "        [43,  1, 51, 39, 63,  1, 40, 43],\n",
      "        [58, 46, 43,  1, 43, 39, 56, 57],\n",
      "        [39, 58, 47, 53, 52, 12,  1, 37],\n",
      "        [53, 56, 43,  1, 21,  1, 41, 39],\n",
      "        [50, 39, 52, 63,  1, 47, 58, 57],\n",
      "        [56, 53, 63,  1, 42, 47, 42,  1],\n",
      "        [39, 51,  1, 39, 44, 56, 39, 47],\n",
      "        [17, 24, 21, 38, 13, 14, 17, 32],\n",
      "        [ 1, 39, 52, 42,  1, 45, 43, 50],\n",
      "        [ 1, 58, 46, 39, 58,  1, 42, 53],\n",
      "        [ 1, 61, 53, 59, 50, 42,  1, 21],\n",
      "        [59, 57, 40, 39, 52, 42,  1, 40],\n",
      "        [52, 42,  8,  0,  0, 23, 21, 26],\n",
      "        [45, 53, 42, 57,  0, 23, 43, 43],\n",
      "        [52,  1, 61, 39, 57,  1, 51, 53],\n",
      "        [39, 49, 12,  1, 27,  1, 58, 56],\n",
      "        [53, 44,  1, 57, 54, 43, 43, 41],\n",
      "        [57, 53, 52, 57,  8,  0,  0, 25],\n",
      "        [ 1, 42, 43, 44, 43, 41, 58,  1],\n",
      "        [21,  1, 61, 39, 52, 42, 43, 56],\n",
      "        [43, 43, 51,  5, 42,  1, 40, 59],\n",
      "        [45, 50, 63,  1, 52, 53, 61, 12],\n",
      "        [52, 53, 58,  8,  0, 25, 63,  1],\n",
      "        [53, 58,  6,  1, 51, 63,  1, 50]]) with shape torch.Size([32, 8])\n",
      "Outputs are tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39],\n",
      "        [43, 60, 43, 52,  1, 63, 43, 39],\n",
      "        [43, 42,  8,  0, 25, 63,  1, 45],\n",
      "        [42,  5, 57,  1, 57, 39, 49, 43],\n",
      "        [57, 58, 63,  6,  1, 58, 46, 47],\n",
      "        [ 1, 51, 39, 63,  1, 40, 43,  1],\n",
      "        [46, 43,  1, 43, 39, 56, 57, 10],\n",
      "        [58, 47, 53, 52, 12,  1, 37, 53],\n",
      "        [56, 43,  1, 21,  1, 41, 39, 51],\n",
      "        [39, 52, 63,  1, 47, 58, 57, 43],\n",
      "        [53, 63,  1, 42, 47, 42,  1, 57],\n",
      "        [51,  1, 39, 44, 56, 39, 47, 42],\n",
      "        [24, 21, 38, 13, 14, 17, 32, 20],\n",
      "        [39, 52, 42,  1, 45, 43, 50, 42],\n",
      "        [58, 46, 39, 58,  1, 42, 53,  1],\n",
      "        [61, 53, 59, 50, 42,  1, 21,  1],\n",
      "        [57, 40, 39, 52, 42,  1, 40, 47],\n",
      "        [42,  8,  0,  0, 23, 21, 26, 19],\n",
      "        [53, 42, 57,  0, 23, 43, 43, 54],\n",
      "        [ 1, 61, 39, 57,  1, 51, 53, 56],\n",
      "        [49, 12,  1, 27,  1, 58, 56, 39],\n",
      "        [44,  1, 57, 54, 43, 43, 41, 46],\n",
      "        [53, 52, 57,  8,  0,  0, 25, 17],\n",
      "        [42, 43, 44, 43, 41, 58,  1, 53],\n",
      "        [ 1, 61, 39, 52, 42, 43, 56,  6],\n",
      "        [43, 51,  5, 42,  1, 40, 59, 56],\n",
      "        [50, 63,  1, 52, 53, 61, 12,  0],\n",
      "        [53, 58,  8,  0, 25, 63,  1, 61],\n",
      "        [58,  6,  1, 51, 63,  1, 50, 53]]) with shape torch.Size([32, 8])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch(\"train\")\n",
    "print(f\"Inputs are {xb} with shape {xb.shape}\")\n",
    "print(f\"Outputs are {yb} with shape {yb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c86a15c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is [24] then most probable output is 43\n",
      "When input is [24, 43] then most probable output is 58\n",
      "When input is [24, 43, 58] then most probable output is 5\n",
      "When input is [24, 43, 58, 5] then most probable output is 57\n",
      "When input is [24, 43, 58, 5, 57] then most probable output is 1\n",
      "When input is [24, 43, 58, 5, 57, 1] then most probable output is 46\n",
      "When input is [24, 43, 58, 5, 57, 1, 46] then most probable output is 43\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43] then most probable output is 39\n",
      "When input is [44] then most probable output is 53\n",
      "When input is [44, 53] then most probable output is 56\n",
      "When input is [44, 53, 56] then most probable output is 1\n",
      "When input is [44, 53, 56, 1] then most probable output is 58\n",
      "When input is [44, 53, 56, 1, 58] then most probable output is 46\n",
      "When input is [44, 53, 56, 1, 58, 46] then most probable output is 39\n",
      "When input is [44, 53, 56, 1, 58, 46, 39] then most probable output is 58\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58] then most probable output is 1\n",
      "When input is [52] then most probable output is 58\n",
      "When input is [52, 58] then most probable output is 1\n",
      "When input is [52, 58, 1] then most probable output is 58\n",
      "When input is [52, 58, 1, 58] then most probable output is 46\n",
      "When input is [52, 58, 1, 58, 46] then most probable output is 39\n",
      "When input is [52, 58, 1, 58, 46, 39] then most probable output is 58\n",
      "When input is [52, 58, 1, 58, 46, 39, 58] then most probable output is 1\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1] then most probable output is 46\n",
      "When input is [25] then most probable output is 17\n",
      "When input is [25, 17] then most probable output is 27\n",
      "When input is [25, 17, 27] then most probable output is 10\n",
      "When input is [25, 17, 27, 10] then most probable output is 0\n",
      "When input is [25, 17, 27, 10, 0] then most probable output is 21\n",
      "When input is [25, 17, 27, 10, 0, 21] then most probable output is 1\n",
      "When input is [25, 17, 27, 10, 0, 21, 1] then most probable output is 54\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54] then most probable output is 39\n",
      "When input is [57] then most probable output is 43\n",
      "When input is [57, 43] then most probable output is 60\n",
      "When input is [57, 43, 60] then most probable output is 43\n",
      "When input is [57, 43, 60, 43] then most probable output is 52\n",
      "When input is [57, 43, 60, 43, 52] then most probable output is 1\n",
      "When input is [57, 43, 60, 43, 52, 1] then most probable output is 63\n",
      "When input is [57, 43, 60, 43, 52, 1, 63] then most probable output is 43\n",
      "When input is [57, 43, 60, 43, 52, 1, 63, 43] then most probable output is 39\n",
      "When input is [60] then most probable output is 43\n",
      "When input is [60, 43] then most probable output is 42\n",
      "When input is [60, 43, 42] then most probable output is 8\n",
      "When input is [60, 43, 42, 8] then most probable output is 0\n",
      "When input is [60, 43, 42, 8, 0] then most probable output is 25\n",
      "When input is [60, 43, 42, 8, 0, 25] then most probable output is 63\n",
      "When input is [60, 43, 42, 8, 0, 25, 63] then most probable output is 1\n",
      "When input is [60, 43, 42, 8, 0, 25, 63, 1] then most probable output is 45\n",
      "When input is [56] then most probable output is 42\n",
      "When input is [56, 42] then most probable output is 5\n",
      "When input is [56, 42, 5] then most probable output is 57\n",
      "When input is [56, 42, 5, 57] then most probable output is 1\n",
      "When input is [56, 42, 5, 57, 1] then most probable output is 57\n",
      "When input is [56, 42, 5, 57, 1, 57] then most probable output is 39\n",
      "When input is [56, 42, 5, 57, 1, 57, 39] then most probable output is 49\n",
      "When input is [56, 42, 5, 57, 1, 57, 39, 49] then most probable output is 43\n",
      "When input is [43] then most probable output is 57\n",
      "When input is [43, 57] then most probable output is 58\n",
      "When input is [43, 57, 58] then most probable output is 63\n",
      "When input is [43, 57, 58, 63] then most probable output is 6\n",
      "When input is [43, 57, 58, 63, 6] then most probable output is 1\n",
      "When input is [43, 57, 58, 63, 6, 1] then most probable output is 58\n",
      "When input is [43, 57, 58, 63, 6, 1, 58] then most probable output is 46\n",
      "When input is [43, 57, 58, 63, 6, 1, 58, 46] then most probable output is 47\n",
      "When input is [43] then most probable output is 1\n",
      "When input is [43, 1] then most probable output is 51\n",
      "When input is [43, 1, 51] then most probable output is 39\n",
      "When input is [43, 1, 51, 39] then most probable output is 63\n",
      "When input is [43, 1, 51, 39, 63] then most probable output is 1\n",
      "When input is [43, 1, 51, 39, 63, 1] then most probable output is 40\n",
      "When input is [43, 1, 51, 39, 63, 1, 40] then most probable output is 43\n",
      "When input is [43, 1, 51, 39, 63, 1, 40, 43] then most probable output is 1\n",
      "When input is [58] then most probable output is 46\n",
      "When input is [58, 46] then most probable output is 43\n",
      "When input is [58, 46, 43] then most probable output is 1\n",
      "When input is [58, 46, 43, 1] then most probable output is 43\n",
      "When input is [58, 46, 43, 1, 43] then most probable output is 39\n",
      "When input is [58, 46, 43, 1, 43, 39] then most probable output is 56\n",
      "When input is [58, 46, 43, 1, 43, 39, 56] then most probable output is 57\n",
      "When input is [58, 46, 43, 1, 43, 39, 56, 57] then most probable output is 10\n",
      "When input is [39] then most probable output is 58\n",
      "When input is [39, 58] then most probable output is 47\n",
      "When input is [39, 58, 47] then most probable output is 53\n",
      "When input is [39, 58, 47, 53] then most probable output is 52\n",
      "When input is [39, 58, 47, 53, 52] then most probable output is 12\n",
      "When input is [39, 58, 47, 53, 52, 12] then most probable output is 1\n",
      "When input is [39, 58, 47, 53, 52, 12, 1] then most probable output is 37\n",
      "When input is [39, 58, 47, 53, 52, 12, 1, 37] then most probable output is 53\n",
      "When input is [53] then most probable output is 56\n",
      "When input is [53, 56] then most probable output is 43\n",
      "When input is [53, 56, 43] then most probable output is 1\n",
      "When input is [53, 56, 43, 1] then most probable output is 21\n",
      "When input is [53, 56, 43, 1, 21] then most probable output is 1\n",
      "When input is [53, 56, 43, 1, 21, 1] then most probable output is 41\n",
      "When input is [53, 56, 43, 1, 21, 1, 41] then most probable output is 39\n",
      "When input is [53, 56, 43, 1, 21, 1, 41, 39] then most probable output is 51\n",
      "When input is [50] then most probable output is 39\n",
      "When input is [50, 39] then most probable output is 52\n",
      "When input is [50, 39, 52] then most probable output is 63\n",
      "When input is [50, 39, 52, 63] then most probable output is 1\n",
      "When input is [50, 39, 52, 63, 1] then most probable output is 47\n",
      "When input is [50, 39, 52, 63, 1, 47] then most probable output is 58\n",
      "When input is [50, 39, 52, 63, 1, 47, 58] then most probable output is 57\n",
      "When input is [50, 39, 52, 63, 1, 47, 58, 57] then most probable output is 43\n",
      "When input is [56] then most probable output is 53\n",
      "When input is [56, 53] then most probable output is 63\n",
      "When input is [56, 53, 63] then most probable output is 1\n",
      "When input is [56, 53, 63, 1] then most probable output is 42\n",
      "When input is [56, 53, 63, 1, 42] then most probable output is 47\n",
      "When input is [56, 53, 63, 1, 42, 47] then most probable output is 42\n",
      "When input is [56, 53, 63, 1, 42, 47, 42] then most probable output is 1\n",
      "When input is [56, 53, 63, 1, 42, 47, 42, 1] then most probable output is 57\n",
      "When input is [39] then most probable output is 51\n",
      "When input is [39, 51] then most probable output is 1\n",
      "When input is [39, 51, 1] then most probable output is 39\n",
      "When input is [39, 51, 1, 39] then most probable output is 44\n",
      "When input is [39, 51, 1, 39, 44] then most probable output is 56\n",
      "When input is [39, 51, 1, 39, 44, 56] then most probable output is 39\n",
      "When input is [39, 51, 1, 39, 44, 56, 39] then most probable output is 47\n",
      "When input is [39, 51, 1, 39, 44, 56, 39, 47] then most probable output is 42\n",
      "When input is [17] then most probable output is 24\n",
      "When input is [17, 24] then most probable output is 21\n",
      "When input is [17, 24, 21] then most probable output is 38\n",
      "When input is [17, 24, 21, 38] then most probable output is 13\n",
      "When input is [17, 24, 21, 38, 13] then most probable output is 14\n",
      "When input is [17, 24, 21, 38, 13, 14] then most probable output is 17\n",
      "When input is [17, 24, 21, 38, 13, 14, 17] then most probable output is 32\n",
      "When input is [17, 24, 21, 38, 13, 14, 17, 32] then most probable output is 20\n",
      "When input is [1] then most probable output is 39\n",
      "When input is [1, 39] then most probable output is 52\n",
      "When input is [1, 39, 52] then most probable output is 42\n",
      "When input is [1, 39, 52, 42] then most probable output is 1\n",
      "When input is [1, 39, 52, 42, 1] then most probable output is 45\n",
      "When input is [1, 39, 52, 42, 1, 45] then most probable output is 43\n",
      "When input is [1, 39, 52, 42, 1, 45, 43] then most probable output is 50\n",
      "When input is [1, 39, 52, 42, 1, 45, 43, 50] then most probable output is 42\n",
      "When input is [1] then most probable output is 58\n",
      "When input is [1, 58] then most probable output is 46\n",
      "When input is [1, 58, 46] then most probable output is 39\n",
      "When input is [1, 58, 46, 39] then most probable output is 58\n",
      "When input is [1, 58, 46, 39, 58] then most probable output is 1\n",
      "When input is [1, 58, 46, 39, 58, 1] then most probable output is 42\n",
      "When input is [1, 58, 46, 39, 58, 1, 42] then most probable output is 53\n",
      "When input is [1, 58, 46, 39, 58, 1, 42, 53] then most probable output is 1\n",
      "When input is [1] then most probable output is 61\n",
      "When input is [1, 61] then most probable output is 53\n",
      "When input is [1, 61, 53] then most probable output is 59\n",
      "When input is [1, 61, 53, 59] then most probable output is 50\n",
      "When input is [1, 61, 53, 59, 50] then most probable output is 42\n",
      "When input is [1, 61, 53, 59, 50, 42] then most probable output is 1\n",
      "When input is [1, 61, 53, 59, 50, 42, 1] then most probable output is 21\n",
      "When input is [1, 61, 53, 59, 50, 42, 1, 21] then most probable output is 1\n",
      "When input is [59] then most probable output is 57\n",
      "When input is [59, 57] then most probable output is 40\n",
      "When input is [59, 57, 40] then most probable output is 39\n",
      "When input is [59, 57, 40, 39] then most probable output is 52\n",
      "When input is [59, 57, 40, 39, 52] then most probable output is 42\n",
      "When input is [59, 57, 40, 39, 52, 42] then most probable output is 1\n",
      "When input is [59, 57, 40, 39, 52, 42, 1] then most probable output is 40\n",
      "When input is [59, 57, 40, 39, 52, 42, 1, 40] then most probable output is 47\n",
      "When input is [52] then most probable output is 42\n",
      "When input is [52, 42] then most probable output is 8\n",
      "When input is [52, 42, 8] then most probable output is 0\n",
      "When input is [52, 42, 8, 0] then most probable output is 0\n",
      "When input is [52, 42, 8, 0, 0] then most probable output is 23\n",
      "When input is [52, 42, 8, 0, 0, 23] then most probable output is 21\n",
      "When input is [52, 42, 8, 0, 0, 23, 21] then most probable output is 26\n",
      "When input is [52, 42, 8, 0, 0, 23, 21, 26] then most probable output is 19\n",
      "When input is [45] then most probable output is 53\n",
      "When input is [45, 53] then most probable output is 42\n",
      "When input is [45, 53, 42] then most probable output is 57\n",
      "When input is [45, 53, 42, 57] then most probable output is 0\n",
      "When input is [45, 53, 42, 57, 0] then most probable output is 23\n",
      "When input is [45, 53, 42, 57, 0, 23] then most probable output is 43\n",
      "When input is [45, 53, 42, 57, 0, 23, 43] then most probable output is 43\n",
      "When input is [45, 53, 42, 57, 0, 23, 43, 43] then most probable output is 54\n",
      "When input is [52] then most probable output is 1\n",
      "When input is [52, 1] then most probable output is 61\n",
      "When input is [52, 1, 61] then most probable output is 39\n",
      "When input is [52, 1, 61, 39] then most probable output is 57\n",
      "When input is [52, 1, 61, 39, 57] then most probable output is 1\n",
      "When input is [52, 1, 61, 39, 57, 1] then most probable output is 51\n",
      "When input is [52, 1, 61, 39, 57, 1, 51] then most probable output is 53\n",
      "When input is [52, 1, 61, 39, 57, 1, 51, 53] then most probable output is 56\n",
      "When input is [39] then most probable output is 49\n",
      "When input is [39, 49] then most probable output is 12\n",
      "When input is [39, 49, 12] then most probable output is 1\n",
      "When input is [39, 49, 12, 1] then most probable output is 27\n",
      "When input is [39, 49, 12, 1, 27] then most probable output is 1\n",
      "When input is [39, 49, 12, 1, 27, 1] then most probable output is 58\n",
      "When input is [39, 49, 12, 1, 27, 1, 58] then most probable output is 56\n",
      "When input is [39, 49, 12, 1, 27, 1, 58, 56] then most probable output is 39\n",
      "When input is [53] then most probable output is 44\n",
      "When input is [53, 44] then most probable output is 1\n",
      "When input is [53, 44, 1] then most probable output is 57\n",
      "When input is [53, 44, 1, 57] then most probable output is 54\n",
      "When input is [53, 44, 1, 57, 54] then most probable output is 43\n",
      "When input is [53, 44, 1, 57, 54, 43] then most probable output is 43\n",
      "When input is [53, 44, 1, 57, 54, 43, 43] then most probable output is 41\n",
      "When input is [53, 44, 1, 57, 54, 43, 43, 41] then most probable output is 46\n",
      "When input is [57] then most probable output is 53\n",
      "When input is [57, 53] then most probable output is 52\n",
      "When input is [57, 53, 52] then most probable output is 57\n",
      "When input is [57, 53, 52, 57] then most probable output is 8\n",
      "When input is [57, 53, 52, 57, 8] then most probable output is 0\n",
      "When input is [57, 53, 52, 57, 8, 0] then most probable output is 0\n",
      "When input is [57, 53, 52, 57, 8, 0, 0] then most probable output is 25\n",
      "When input is [57, 53, 52, 57, 8, 0, 0, 25] then most probable output is 17\n",
      "When input is [1] then most probable output is 42\n",
      "When input is [1, 42] then most probable output is 43\n",
      "When input is [1, 42, 43] then most probable output is 44\n",
      "When input is [1, 42, 43, 44] then most probable output is 43\n",
      "When input is [1, 42, 43, 44, 43] then most probable output is 41\n",
      "When input is [1, 42, 43, 44, 43, 41] then most probable output is 58\n",
      "When input is [1, 42, 43, 44, 43, 41, 58] then most probable output is 1\n",
      "When input is [1, 42, 43, 44, 43, 41, 58, 1] then most probable output is 53\n",
      "When input is [21] then most probable output is 1\n",
      "When input is [21, 1] then most probable output is 61\n",
      "When input is [21, 1, 61] then most probable output is 39\n",
      "When input is [21, 1, 61, 39] then most probable output is 52\n",
      "When input is [21, 1, 61, 39, 52] then most probable output is 42\n",
      "When input is [21, 1, 61, 39, 52, 42] then most probable output is 43\n",
      "When input is [21, 1, 61, 39, 52, 42, 43] then most probable output is 56\n",
      "When input is [21, 1, 61, 39, 52, 42, 43, 56] then most probable output is 6\n",
      "When input is [43] then most probable output is 43\n",
      "When input is [43, 43] then most probable output is 51\n",
      "When input is [43, 43, 51] then most probable output is 5\n",
      "When input is [43, 43, 51, 5] then most probable output is 42\n",
      "When input is [43, 43, 51, 5, 42] then most probable output is 1\n",
      "When input is [43, 43, 51, 5, 42, 1] then most probable output is 40\n",
      "When input is [43, 43, 51, 5, 42, 1, 40] then most probable output is 59\n",
      "When input is [43, 43, 51, 5, 42, 1, 40, 59] then most probable output is 56\n",
      "When input is [45] then most probable output is 50\n",
      "When input is [45, 50] then most probable output is 63\n",
      "When input is [45, 50, 63] then most probable output is 1\n",
      "When input is [45, 50, 63, 1] then most probable output is 52\n",
      "When input is [45, 50, 63, 1, 52] then most probable output is 53\n",
      "When input is [45, 50, 63, 1, 52, 53] then most probable output is 61\n",
      "When input is [45, 50, 63, 1, 52, 53, 61] then most probable output is 12\n",
      "When input is [45, 50, 63, 1, 52, 53, 61, 12] then most probable output is 0\n",
      "When input is [52] then most probable output is 53\n",
      "When input is [52, 53] then most probable output is 58\n",
      "When input is [52, 53, 58] then most probable output is 8\n",
      "When input is [52, 53, 58, 8] then most probable output is 0\n",
      "When input is [52, 53, 58, 8, 0] then most probable output is 25\n",
      "When input is [52, 53, 58, 8, 0, 25] then most probable output is 63\n",
      "When input is [52, 53, 58, 8, 0, 25, 63] then most probable output is 1\n",
      "When input is [52, 53, 58, 8, 0, 25, 63, 1] then most probable output is 61\n",
      "When input is [53] then most probable output is 58\n",
      "When input is [53, 58] then most probable output is 6\n",
      "When input is [53, 58, 6] then most probable output is 1\n",
      "When input is [53, 58, 6, 1] then most probable output is 51\n",
      "When input is [53, 58, 6, 1, 51] then most probable output is 63\n",
      "When input is [53, 58, 6, 1, 51, 63] then most probable output is 1\n",
      "When input is [53, 58, 6, 1, 51, 63, 1] then most probable output is 50\n",
      "When input is [53, 58, 6, 1, 51, 63, 1, 50] then most probable output is 53\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"When input is {context.tolist()} then most probable output is {target.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b65597e",
   "metadata": {},
   "source": [
    "### Baseline mode - Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e9cc609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ffdd55661f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fce90c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37df3af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "\n",
    "        k = self.key(x) # (B,T,16)\n",
    "        q = self.query(x) # (B,T,16)\n",
    "\n",
    "        # compute attention score (\"affinities to other tokens around\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B,T,16) @ (B,16,T) => (B,T,T)\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf')) # (B,T,T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B,T,T)\n",
    "\n",
    "        # perform the aggrrgation\n",
    "        v = self.value(x) # (B,T,C)\n",
    "\n",
    "        out = wei @ v # (B,T,T) @ (B,T,C) => (B,T,C)\n",
    "\n",
    "        out.shape\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4a6b5ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BiGramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # each token directly reads the logits of the next token from lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        # position embedding table\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        # self attention head\n",
    "        self.sa_head = Head(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "        \n",
    "        # idx and target are both (B,T) tensors of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C) => (Batch, Time, Channel=n_embed) => (4,8,vocab_size)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        \n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.sa_head(x) # apply one head of self attention (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        # cross entropy expects the output as (B, C, T), so we need to reshape\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # we need to crop idx because our embedding size is limited to block size now\n",
    "            idx_cropped = idx[:,-block_size:]\n",
    "            \n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cropped)\n",
    "            # focus only on the last tim step\n",
    "            logits = logits[:,-1,:] # becomes (B,C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # (B,1)\n",
    "            # append sampled index to running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B,T+1)\n",
    "        return idx\n",
    "    \n",
    "model = BiGramLanguageModel()\n",
    "m = model.to(device)\n",
    "# logits, loss = m(xb,yb)\n",
    "# print(logits.shape)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d754485a",
   "metadata": {},
   "source": [
    "We can actually predict the log liklihood of the data if we know the vocab size, it is `-ln(1/vocab_size)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b3db51",
   "metadata": {},
   "source": [
    "#### Now let us create the model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1926ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81ab46af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2497, val loss 4.2480\n",
      "step 1000: train loss 2.5221, val loss 2.5427\n",
      "step 2000: train loss 2.4438, val loss 2.4740\n",
      "step 3000: train loss 2.4170, val loss 2.4383\n",
      "step 4000: train loss 2.3873, val loss 2.4235\n",
      "step 5000: train loss 2.3862, val loss 2.4076\n",
      "step 6000: train loss 2.3802, val loss 2.3992\n",
      "step 7000: train loss 2.3738, val loss 2.4066\n",
      "step 8000: train loss 2.3606, val loss 2.3987\n",
      "step 9000: train loss 2.3484, val loss 2.3955\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d71bac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ins iees yofrickild che om 't youtfour my sesm heur cech heat my bug ho, lay theich aw harsemanoollt, theed\n",
      "An onovoliatclonone eces San hiceato,\n",
      "Tonrut ourefast had heesat to ono our yo ten hendety njeie by, sto!\n",
      "DULINAS:\n",
      "Her loure by ciray?\n",
      "\n",
      "\n",
      "Yowme Whar;\n",
      "As, sofry wow, aig h wis the mseshe che indy the laschint outr atr, hour foliroshe. Winga nd! I h'treitheesour fain:\n",
      "Se\n",
      "thant thas yo wowe od, kint yod this.\n",
      "The alke Iis, tre thowoum gbe migortald honwios beat wher histt amat se\n",
      "t cut, ithan \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx=torch.zeros((1,1), dtype = torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110cfc9",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe4e89",
   "metadata": {},
   "source": [
    "In the last model we created (Bigram model), there was no way for the character level tokens to talk to their ancestors. They all were generated based on just the previous token. \n",
    "\n",
    "To begin with lets assume that token at `ith` location interacts with all the tokens before that. For this behavior we take the mean of sum of interactions of `ith` element with all the elements before that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fe6c2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fce74c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1\n",
    "# xbow is x bag of words\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] #(t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "502e4ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "899d9d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95688ba4",
   "metadata": {},
   "source": [
    "The above method is inefficient as we are doing it in n^2 complexity, we can use matrix multiplication to lower the time complexity. We will use the lower triagular matrix and make it more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6fb677c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2\n",
    "\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x\n",
    "# both the torch vectors are same, this is the more efficient way\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64a5f237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f31b0710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4\n",
    "# Self attention !\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape\n",
    "\n",
    "# single head perform self attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B,T,16)\n",
    "q = query(x) # (B,T,16)\n",
    "\n",
    "wei = q @ k.transpose(-2,-1) # (B,T,16) @ (B,16,T) => (B,T,T)\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "\n",
    "# wei is equivalent to the np.dot(Q,K) \n",
    "#wei comes from Q.K_transpose\n",
    "# wei = torch.zeros((T,T))\n",
    "\n",
    "# this line of code prevents future nodes to communicate information to past nodes. If removed every node(token)\n",
    "# will interact with each other. This is removed on the encoder side of the transformers.\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bbaddb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f195b4",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "1. Attention is a communication mechanism\n",
    "2. There is no notion of space, they are just vectors without any idea of dimension thats why we add positional encoding.\n",
    "3. Every batch is independently trained and they never talk to each other.\n",
    "4. \"Self attention\" - The key, query and value matrix are all coming from the same source.\n",
    "5. \"Cross attention\" - When we pull matrix key and value from different nodes, this is called cross attention.\n",
    "6. If we have unit gaussian input when we set our matrix, the variance of the resultant matrix will be of the order of head_size which is far from when we had started setting the matrix up. If we multiply by sqrt(head_size) then we preserve information. This is important so that the the result after softmax is not sharpened towards the maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36067e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
