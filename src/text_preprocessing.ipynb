{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d08ba13a",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "It is a very important step that helps to reduce the complexity of the raw text and helps the in future tasks. Proper care should be taken in the preprocessing as it might also lead to loss of important information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe26eec9",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "It is used to transform the word into its most basic form. Eg. jumping, jumps and jumped can be transformed to its stemmed form \"jump\". This reduces the total number of words that are required to be stored in the corpus. It might also lead to errors in the output, depends on the problem that we are trying to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4386fe6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adityasingh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d8432b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1985cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8c79635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cat\n",
      "walk\n",
      "walk\n",
      "achiev\n",
      "am\n",
      "is\n",
      "are\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem(\"cat\")) # -> cat\n",
    "print(stemmer.stem(\"cats\")) # -> cat\n",
    "\n",
    "print(stemmer.stem(\"walking\")) # -> walk\n",
    "print(stemmer.stem(\"walked\")) # -> walk\n",
    "\n",
    "print(stemmer.stem(\"achieve\")) # -> achiev\n",
    "\n",
    "print(stemmer.stem(\"am\")) # -> am\n",
    "print(stemmer.stem(\"is\")) # -> is\n",
    "print(stemmer.stem(\"are\")) # -> are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50246241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'are', 'sleep', '.', 'what', 'are', 'the', 'dog', 'do', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"The cats are sleeping. What are the dogs doing?\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "tokens_stemmed = [stemmer.stem(token) for token in tokens]\n",
    "print(tokens_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f923a0ff",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "It is similar to stemming that brings the word to their base form. The only difference being that lemmatization uses the morphological analysis of the word. The word \"better\" has \"good\" as its base word which will be correctly identified by lemmatization but not by stemming. It is computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c15c60c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/adityasingh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/adityasingh/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/adityasingh/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec96623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b8b669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbacf4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cat\n",
      "walking\n",
      "walked\n",
      "achieve\n",
      "am\n",
      "is\n",
      "are\n",
      "better\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"cat\")) # -> cat\n",
    "print(lemmatizer.lemmatize(\"cats\")) # -> cat\n",
    "\n",
    "print(lemmatizer.lemmatize(\"walking\")) # -> walk\n",
    "print(lemmatizer.lemmatize(\"walked\")) # -> walk\n",
    "\n",
    "print(lemmatizer.lemmatize(\"achieve\")) # -> achiev\n",
    "\n",
    "print(lemmatizer.lemmatize(\"am\")) # -> am\n",
    "print(lemmatizer.lemmatize(\"is\")) # -> is\n",
    "print(lemmatizer.lemmatize(\"are\")) # -> are\n",
    "print(lemmatizer.lemmatize(\"better\"))\n",
    "print(lemmatizer.lemmatize(\"good\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f660f54",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "These are the words that are pretty common in the language and dont carry any special weight in the language related tasks like a, an, the, is, in ,etc. Although, they might play a significant role in some tasks like sentiment analysis, so whether to remove stop words should depend on the task as hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88f5ebc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adityasingh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0141d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cda233c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "print(len(english_stopwords))\n",
    "print(english_stopwords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1408351",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "This is generally the first step in the NLP task. It breaks the textual data in token which may be words or sentences or something else (the basic unit in that problem). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beea7646",
   "metadata": {},
   "source": [
    "### Lowercase\n",
    "\n",
    "In this the textual data is completely taken to the lower case. It helps to reduce the corpus, like the words \"India\", \"INDIA\" will be converted to \"india\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4ae87d",
   "metadata": {},
   "source": [
    "### Punctuation removal\n",
    "\n",
    "Removing punctuation from the data. It might provide additional context in different problem type. It must only be removed from the data if it is not required at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5ea72d",
   "metadata": {},
   "source": [
    "### Spell correction\n",
    "\n",
    "Spell check and correction are essential for identifying and fixing typos and spelling errors in text data. This process helps reduce redundancy by ensuring that words like \"speling\" and \"spelling\" are recognized as the same word after correction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbabcec2",
   "metadata": {},
   "source": [
    "### Noise removal\n",
    "\n",
    "Noise removal is the process of eliminating unwanted characters, digits, and text fragments that can disrupt text analysis. This can include removing headers, footers, HTML, and XML content.\n",
    "\n",
    "### Text normalization\n",
    "\n",
    "Text normalization involves standardizing text by converting it to the same case (usually lowercase), removing punctuation, and converting numbers to their word forms.\n",
    "\n",
    "### POS tagging\n",
    "\n",
    "Part-of-speech tagging identifies the grammatical category (noun, verb, adjective, etc.) of each word in a sentence. It is valuable for understanding sentence structure and aids in tasks like named entity recognition and question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "727386c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e91bac77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), (',', ','), ('my', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Aditya', 'NNP'), ('Singh', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, my name is Aditya Singh\"\n",
    "word_tokenize = nltk.word_tokenize(text)\n",
    "print(nltk.pos_tag(word_tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4351122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('love', 'VBP'), ('my', 'PRP$'), ('country', 'NN'), ('that', 'WDT'), ('is', 'VBZ'), ('India', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "text = \"I love my country that is India\"\n",
    "word_tokenize = nltk.word_tokenize(text)\n",
    "print(nltk.pos_tag(word_tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ae0a84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
